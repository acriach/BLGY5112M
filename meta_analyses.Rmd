---
title: "Meta-Analyses"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, comment = NA)
```

![alt.text=Two diagrams. Left diagram has three silouhettes of insects above a graph with four horizontal lines with points in the middle of each labbelled female, male mixed sex and hermaphrodite. None of the four lines overlap the reference vertical dotted line. Right diagram has seven silouhettes of insects above a graph with four horizontal lines with points in the middle of each labbelled female, male mixed sex and hermaphrodite. On this graph all four lines overlap the reference vertical dotted line.](./images/metanalysis.jpg)

There are many steps involve in a meta-analysis. For guidance on how to do a meta-analysis see [(Nakagawa *et al.* 2017)](https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-017-0357-7) which divides the process of meta-analysis into 10 questions.

This lesson only considers how to do the statistics part of the meta-analysis & publication bias. 

<br>

# Calculating effect sizes

## `Metafor` for meta-analysis

First, install and load the `metafor` package.

```{r, tidy=TRUE, message=FALSE, warning=FALSE}
library(metafor)
```

<br>

Have a look at the data set named `dat.curtis1998` included in the package.

```{r}
dat <- dat.curtis1998
str(dat)
```

<br>

This data set is from the paper by Curtis and Wang (1998). They looked at the effect of increased CO$_2$ on plant traits (mainly changes in biomass). There is experimental details (sometimes called moderators) including species and functional group. There is also means (m), standard deviations (sd) and sample sizes (n) for the control group (1) and experimental group (2) in the last few columns.

![alt.text=A photo of tree trunks, bushes and ferns.](./images/forest.jpg)

<br>

## Calculating 'standardized' effect sizes

To compare the effect of increased CO$_2$ across multiple studies, we first need to calculate an **effect size** for each study - a metric that quantifies the difference between our control and experimental groups.

:::: {.cadetbluebox data-latex=""}
::: {.center data-latex=""}
:::

Effect size reminder. If experiment 1 measures the weights of swans and geese and experiment 2 measures the weight of swans and robins, the effect size is bigger in experiment 2. There is a bigger effect of species on weight in experiment 2. 
::::

<br>

There are several 'standardized' effect sizes. When we have two groups to compare, we have a choice of two effect size statistics we could use. 

1) The first choice is standardized mean difference (SMD also known as Cohen's $d$ or Hedge's $d$ or $g$; there are some subtle differences between d and g, but we do not worry about them for now.)

<details>
  <summary>**Formula for calculating SMD**</summary>

In the formula below:  
* $\bar{x}_{C}$ and $\bar{x}_{E}$ are the means of the control and experimental group, respectively  
* $sd$ is sample standard deviation    
* $n$ is sample size  

\begin{equation}
\mathrm{SMD}=\frac{\bar{x}_{E}-\bar{x}_{C}}{\sqrt{\frac{(n_{C}-1)sd^2_{C}+(n_{E}-1)sd^2_{E}}{n_{C}+n_{E}-2}}}
\end{equation}

</details>

<br>

We also need to calculate the SMD's sample error variance for each study.

<details>
  <summary>**Formula for calculating sample error variance**</summary>

\begin{equation}
se^2_{\mathrm{SMD}}= \frac{n_{C}+n_{E}}{n_{C}n_{E}}+\frac{\mathrm{SMD}^2}{2(n_{C}+n_{E})}
\end{equation}

</details>

<br>

The square root of this is referred to as 'standard error'. The inverse of this (the inverse of a number is when you divide 1 by it e.g.$1/se^2$) is used as 'weight' (studies with bigger sample sizes will have bigger 'weight' in the analysis) 

<br>

2) The second option of standardised effect size is called 'response ratio', which is usually presented in its natural logarithm form (lnRR).

<details>
  <summary>**Formula for lnRR**</summary>

\begin{equation}
\mathrm{lnRR}=\ln\left({\frac{\bar{x}_{E}}{\bar{x}_{C}}}\right)
\end{equation}

</details>

<br>

The sampling error variance for lnRR is also needed.

<details>
  <summary>**Formula for calculating sample error variance for lnRR**</summary>

\begin{equation}
se^2_\mathrm{lnRR}=\frac{sd^2_{C}}{n_{C}\bar{x}^2_{C}}+\frac{sd^2_{E}}{n_{E}\bar{x}^2_{E}}
\end{equation}

</details>

<br>

We can get R to calculate these numbers for each study using the function `escalc()` in `metafor`. To obtain the standardised mean difference (SMD), we use:

```{r effect-size, tidy=TRUE}
# SMD
SMD <- escalc(
  measure = "SMD", n1i = dat$n1i, n2i = dat$n2i,
  m1i = dat$m1i, m2i = dat$m2i,
  sd1i = dat$sd1i, sd2i = dat$sd2i
)
```

<details>
  <summary>**Code Explained**</summary>

Note: in the example dataset `dat` the columns for the sample sizes, means and standard deviation have been given the same name as the arguments that are used in `escalc()`

* `n1i` and `n2i` are the sample sizes
* `m1i` and `m2i` are the means
* `sd1i` and `sd2i` the standard deviations from each study

</details>

<br>

The `SMD` object created has an effect size (yi) and its variance (vi) for each study.

```{r,echo=F}
head(SMD)
```

<br>

To obtain the alternative response ratio (lnRR), we would change the `measure =`:
```{r effect-size-2, tidy=TRUE}
lnRR <- escalc(
  measure = "ROM", n1i = dat$n1i, n2i = dat$n2i,
  m1i = dat$m1i, m2 = dat$m2i,
  sd1i = dat$sd1i, sd2i = dat$sd2i
)
```

<br>

The original paper used lnRR so we will use it, but you will repeat the analysis in the **Challenge** below using SMD to see whether results are consistent.

<br>

Add the effect sizes to the original data set with `bind_cols()` from the package `dplyr` (some people use `cbind()` for this).

```{r, tidy=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
dat <- bind_cols(dat, lnRR)
```

<br>

You should see yi (effect size) and vi (sampling variance) are added.

```{r,echo=F}
str(dat)
```

<br>

## Forest Plots 

Forest plots are a common way of visualising the effect sizes and their 95% confidence intervals, also known as CIs, (based on sampling error variance) for each of the studies in the meta-analysis. The `forest()` function can create this plot.  

```{r forest, tidy=TRUE}
forest(dat$yi, dat$vi)
```
<br>

Unless you have a large screen, you may not be able to see the detail in this forest plot.  Let's look at just the first 12 studies. 

```{r forest2, tidy=TRUE}
forest(dat$yi[1:12], dat$vi[1:12])
```

<br>

:::: {.cadetbluebox data-latex=""}
::: {.center data-latex=""}
:::

We can calculate many different kinds of effect sizes with <kbd>`escalc`</kbd>; other common effect size statistics include $Zr$ (Fisher's z-transformed correlation) which you would use if the meta-analysis was analysing studies that reported correlations oppose to comparing two groups.
  
::::

<br>

**Challenge**

Do an internet search to find out how to interpret forest plots. What do the squares mean? Why is there a dotted line at 0? What does the diamond that you see on some forest plots represent?

<br>

**Challenge**

Now add the SMD values (the alternative ones to lnRR) to `dat` and create a forest plot with them. Compare the two forest plots.

<details>
  <summary>**Solution**</summary>

Use the SMD object created above. Add the vi and yi columns in SMD to the dataset dat.
```{r}
dat <- bind_cols(dat, SMD)
```

If you view dat or run `str(dat)` you will see R has renamed the `vi` and `yi` columns. This means you have the lnRR values and then the SMD values.
```{r}
str(dat)
```

Now create a forest plot.
```{r}
forest(dat$yi...23, dat$vi...24)
```

If you want to view the previous forest plot for comparison try the blue arrow under the `Plots` tab. Alternative you could assign the plots and use that name to call them and view them.
```{r, eval = FALSE}
plotlnRR <- forest(dat$yi...21, dat$vi...22)
plotlnRR
plotSMD <- forest(dat$yi...23, dat$vi...24)
plotSMD
```

Comparing the forest plots, there does seem to be differences. Keep in mind that you may want to investigate how using lnRR or SMD may or may not affect the analysis further on.

</details>


<br>

<br>


# Calculating the overall effect size

After calculating effect sizes, we can run statistical models to estimate mean effect size and the influence of moderator variables.

There are two main models for meta-analysis: 1) the fixed-effect model and 2) the **random-effect** model (actually there's 3 types, but the third one is the extension of the second model). Because fixed effects mean something different in another statistical context, this naming is a bit confusing. So people now call the first model **'common-effect'** model. 

:::: {.cadetbluebox data-latex=""}
::: {.center data-latex=""}
:::

Section Q4 and Fig. 4 in [Nakagawa et al. 2017](https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-017-0357-7) gives a longer explanation and visual representation of the different models.
  
::::

<br>

## Common-effect model

This model estimates the overall mean while considering weights. Weights are used so that different studies with smaller or larger sample sizes have less or more influence in the calculation of the overall effect size.

<details>
  <summary>**Common-effect model formula and explanation**</summary>

\begin{equation}
y_i=b_0+e_i,
\\
e_i\sim \mathcal{N}(0,v_i),
\end{equation}

where $y_i$ is the $i$th effect size (from $i$th study), $b_0$ is the overall mean (or meta-analytic mean), $e_i$ is a deviation from the overall mean. $e_i$ is equivalent to a normal distribution with a mean of 0 and variance of $v_i$. $v_i$ is the study specific sampling variance. Note that weights for this model are $1/v_i$.

</details>

<br>

This model assumes that all the studies sampled from the same population and therefore there is a common mean for all studies. For example, all studies used the same species. This is rare in meta-analysis as the Curtis and Wang 1998 data shows where studies span many different species.

<br>

## Random-effects model

A random-effect model does not make this assumption and therefore can be used when studies have been sampled from different populations.

<details>
  <summary>**Random-effect model formula and explanation**</summary>

\begin{equation}
y_i=b_0+s_i+e_i,
\\
s_i\sim \mathcal{N}(0,\tau^2),\
\\
e_i\sim \mathcal{N}(0,v_i),
\end{equation}

where $s_i$ is a study-specific deviation from the overall mean for $i$th study. As the second formula indicates, $s_i$ is normally distributed with the between-study variance which is  $\tau^2$. Note that weights for this model are $1/(\tau^2+v_i)$. We revisit this point. 

</details>

<br>

Unlike the common-effect model, a random-effect model assumes that different studies have different population means. 

<br>

## Running a common-effect model

Let's use the function `rma` from `metafor` to run a common-effect model using the effect sizes `yi` and variances `vi` we calculated earlier. (Note: make sure those columns are still called `yi` and `vi`.)

```{r re-library, echo = FALSE, message = FALSE, warning = FALSE}
library(metafor)
library(dplyr)
dat <- get(data(dat.curtis1998))
lnRR <- escalc(
  measure = "ROM", n1i = dat$n1i, n2i = dat$n2i,
  m1i = dat$m1i, m2 = dat$m2i,
  sd1i = dat$sd1i, sd2i = dat$sd2i
)
dat <- bind_cols(dat, lnRR)
```

```{r common, tidy=TRUE}
common_m <- rma(yi = yi, vi = vi, method = "FE", data = dat)
```

We specify the effect size (yi), its variance (vi), the method ("FE" for fixed-effect) and the data frame (dat).

<br>

To see the output, use `summary` on the model object:

```{r}
summary(common_m)
```

The overall mean is statistically significant (under `Model Results` look at `pval`). This indicates it is significantly different from 0 and therefore there is an effect of the CO$_2$ treatment on plant biomass. 

The overall mean is under `estimate` and it's around 0.2. What does 0.2 mean? The effect sizes were response ratios on a logarithmic scale (lnRR). We can use `exp()` to convert this back into a response ratio of the control and experimental means. 

```{r back, tidy=TRUE}
exp(0.2)
```

We can say that the plant trait (i.e. biomass) was 22% larger in the experimental group (RR$=\bar{x}_{E}/\bar{x}_{C}$), which is a pretty large effect (remember to interpret results in a biological meaningful way). 

<br>

## Running a random-effects model

Now, we move onto the random-effects model - a more realistic model because these studies were on different species. Again, we use the `rma` function, but this time change the method to REML which is the default and the best method for the random-effect meta-analysis.

```{r random, tidy=TRUE}
random_m <- rma(yi = yi, vi = vi, method = "REML", data = dat)
summary(random_m)
```

Compare the overall mean from this model with the common-effect model. Oh, the overall mean of the random-effects model is actually bigger than that of the fixed-effect model! OK, that sometimes happens (we will find out that this probably is an over-estimation later in the publication bias section). 

We expect the 95% CI (under `ci.lb` and `ci.ub`) to be wider (i.e. more realistic) in this random-effects model as this model has a better assumption than the common-effect model. 

<br>

## Understanding heterogeneity

There are other numbers in the output. We have `tau^2` ($\tau^2$) and `I^2` ($I^2$), two very common measures of heterogeneity (note that `H^2`, or $H^2$ is a transformation of $I^2$). 

:::: {.cadetbluebox data-latex=""}
::: {.center data-latex=""}
:::

Heterogeneity is variation in effect sizes, which is not accounted for by the sampling error variance/random chance. In other words, how consistent the results are across all the studies. This is real variation in the data.

::::

<br>

$I^2$ is an important index as it can tell the percentage of real variation in your meta-analytic data.

<details>
  <summary>**Formula for $I^2$**</summary>

\begin{equation}
I^2=\frac{\tau^2}{(\tau^2+\bar{v})},
\end{equation}

where $\bar{v}$ is a representative value of $v_i$ (or think $\bar{v}$ as the average of $v_i$ although it is not quite it). Note that the denominator is the whole variance which exists in the data. 

</details>

<br>

The benchmark values for $I^2$ are 25, 50 and 75% for low, moderate and high heterogeneity, respectively ([Higgins et al., 2003](https://pubmed.ncbi.nlm.nih.gov/12958120/).)

Our $I^2$ value is 88.9% so very high. The output also shows a `Test for Heterogeneity` or a $Q$ test. As you might expect, $I^2$ is statistically significant meaning there is heterogeniety.

<br>

[Senior et al. 2016](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecy.1591) did a meta-analysis of meta-analyses looking at the average value of $I^2$ in the field of ecology and evolution. The average value was 92%! This indicates that we usually need to fit the random-effects model rather than the common-effect model because the latter assumes heterogeneity to be zero or $\tau^2=0$ and $I^2 = 0$. Or is it really? We find this out later. 

<br>

# Meta-regression (the random-effects model)

The existence of heterogeneity sets a scene for meta-regression. This means that we now put predictors ('moderators' in the meta-analytic terminology) into our model to explain heterogeneity (equivalent to normal regression models).

In this example, let's fit three moderators that were collected by the authors: 1) `time` (how long the experiment was), 2) `method` (different ways of increasing CO$_2$), and 3) `fungroup` (functional group, i.e., angiosperm, gymnosperm or N$_2$ fixer).

We use `rma()` again, but add a model statement.

```{r regression, tidy=TRUE}
metareg <- rma(yi = yi, vi = vi, mod = ~ time + method + fungrp, method = "REML", data = dat)
summary(metareg)
```

Look at the R$^2$ value - the moderators do not explain anything!  Also, the `Test of Moderators` (again the Q value) say they are not significant. A terrible model! So we give up here (in a real meta-analysis, you need to do this more systematically, preferably based on your priori hypotheses). 

<br>

# Checking for publication bias

It seems like an CO$_2$ increase promotes plant growth, but this is assuming the data set we have does not suffer from **publication bias**. 

:::: {.cadetbluebox data-latex=""}
::: {.center data-latex=""}
:::

Publication bias in its simplest form is that significant results are more likely to be published than non-significant results

::::

<br>

There are several methods people use to assess if there is publication bias. Two commonest methods, often used as a set, are: 1) funnel plot, which one uses to detect a funnel asymmetry (a sign of publication bias), and 2) Egger's regression test with which you test funnel asymmetry statistically.

<br>

## Funnel plot

To create a funnel plot:

```{r funnel, tidy=TRUE}
funnel(random_m)
```

The x axis is effect size. The overall effect size is plotted as a dotted vertical line. Each point shows a study's effect size and standard error on the y axis. Note the y axis has 0 at the top.

What am I talking about by 'funnel asymmetry'? We expect to see a symmetrical up-side-down funnel, where effect sizes with low $se$ are more tightly clustered than effect sizes with high $se$. But if we have publication bias, we should see funnel asymmetry. This is because studies with small sample sizes (i.e. high $se$, which leads to non-significance) are less likely to be published.

<br>

## Egger's test

To run Egger's test:

```{r egger, tidy=TRUE}
# Note that the orignal Egger's test is regtest(random_m, model="lm")
regtest(random_m)
```

The Egger's test p value is significant suggesting asymetry. But we need to be careful. Funnel asymmetry can be caused not only by publication bias, but also by heterogeneity (one or more undetected moderators are distorting a funnel shape). Given we have a lot of unexplained variance (i.e. heterogeneity), we cannot be sure what is causing this asymmetry. 

<br>

## Trim-and-fill

We can use the alternative trim-and-fill method through the function `trimfill()`. We get a funnel plot by passing the result through `funnel()`

```{r trimfill, tidy=TRUE}
tf_m <- trimfill(random_m)
tf_m
funnel(tf_m)
```

As you can see this method uses the asymmetry to add more points and provide a revised overall mean, which is smaller than that of the original random-effect model. Although this effect is still significant, this method could turn a significant overall mean into a non-significant one. But rather than taking this as a real estimate of the overall mean, we need to see this as a part of our sensitivity analysis. 

<br>

:::: {.cadetbluebox data-latex=""}
::: {.center data-latex=""}
:::

Sensitivity analyses involve various statistical methods that test how the overall effect size changes depending on the decisions made during the meta-analysis. These decisions will have affected what studies and data was included in the calculation.

::::

<br>

There are more methods for publication bias tests, none of which are perfect, but it is important to do some of these tests (for more see [Nakagawa et al., 2017](https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-017-0357-7) and references therein). 

<br>

# Further help and references

Worked examples on the [metafor package's website](http://www.metafor-project.org/doku.php). 

<p style="margin-left:.5in;text-indent:-.5in">Curtis, P. S., and X. Z. Wang. 1998. A meta-analysis of elevated CO2 effects on woody plant mass, form, and physiology. *Oecologia* 113:299-313.</p>

<p style="margin-left:.5in;text-indent:-.5in">Henmi, M., and J. B. Copas. 2010. Confidence intervals for random effects meta-analysis and robustness to publication bias. *Statistics in Medicine* 29:2969-2983.</p>

<p style="margin-left:.5in;text-indent:-.5in">Higgins, J. P. T., S. G. Thompson, J. J. Deeks, and D. G. Altman. 2003. Measuring inconsistency in meta-analyses. *British Medical Journal* 327:557-560.</p>

<p style="margin-left:.5in;text-indent:-.5in">Nakagawa, S., R. Poulin, K. Mengersen, K. Reinhold, L. Engqvist, M. Lagisz, and A. M. Senior. 2015. Meta-analysis of variation: ecological and evolutionary applications and beyond. *Methods in Ecology and Evolution* 6:143-152.</p>

<p style="margin-left:.5in;text-indent:-.5in">Nakagawa, S., D. W. A. Noble, A. M. Senior, and M. Lagisz. 2017. Meta-evaluation of meta-analysis: ten appraisal questions for biologists. *BMC Biology* 15:18.</p>

<p style="margin-left:.5in;text-indent:-.5in">Senior, A. M., C. E. Grueber, T. Kamiya, M. Lagisz, K. O'Dwyer, E. S. A. Santos, and S. Nakagawa. 2016. Heterogeneity in ecological and evolutionary meta-analyses: its magnitude and implications. *Ecology* 97:3293-3299.</p>

<p style="margin-left:.5in;text-indent:-.5in">Viechtbauer, W. 2010. Conducting meta-analyses in R with the metafor package. *Journal of Statistical Software* 36:1-48.</p>

<br>

# Challenges

Use the [meta analysis by Li et al., (2002)](https://doi.org/10.1016/j.jhazmat.2021.127211) which looks at stress in bivalves caused by microplastics using various measures.

Li et al., (2022). Is microplastic an oxidative stressor? Evidence from a meta-analysis on bivalves. Journal of Hazardous Materials, 423, 127211. DOI: https://doi.org/10.1016/j.jhazmat.2021.127211

**Challenge 1**

To understand how means and standard deviations are collated, look at [the study below](https://doi.org/10.1016/j.ecoenv.2020.110871) which is just one of the 25 studies used in Li et al.'s meta analysis.  Try to find the values that were used for the measurement SOD (superoxide dismutase).

Webb, S., Gaw, S., Marsden, I. D., & McRae, N. K. (2020). Biomarker responses in New Zealand green-lipped mussels Perna canaliculus exposed to microplastics and triclosan. Ecotoxicology and Environmental Safety, 201, 110871. DOI: https://doi.org/10.1016/j.ecoenv.2020.110871

<details>
  <summary>**Answer**</summary>

You may have looked in the "Oxidative stress" part of the Results section or Fig 4B but found that the means and standard deviation are not given. There is a link under "Appendix A.Supplementary data". Following this link gives the message "Data will be made available on request". While it is preferable to contact authors to obtain accurate data, people sometimes resort to using image analysis software to calculate means and standard deviations from high resolution graphs. The authors of the meta-analysis explain:

> "When the raw data of the experiments could not be found in the article, numerical values were extracted from the graphs by a digital ruler (Plot Digitizer 2.6.8)." 

</details>

<br>

**Challenge 2**

On the [webpage for Li et al.'s meta analysis](https://doi.org/10.1016/j.jhazmat.2021.127211), find the Appendix A Supplementary material section and download the raw data used in the meta analysis.

Read the sheet for SOD into R.

<details>
  <summary>**Tips**</summary>

Be aware there are multiple workbooks in the excel file, use only the SOD one. You will need to save a copy of the file.  
Decide what format to save the file as, for example, csv or excel.  
Remember the file format determines what function you use in R.   
Once read in, check the data looks as you would expect by viewing it or using `head()`.  
Consider changing the names of the variables.

</details>

<br>

**Challenge 3**

Using the methods and code for the CO^2^ and plant example above, carry out a meta-analysis on the bivalve SOD data. If you get as far as doing a meta-regression you could use particle size, species or exposure time as moderators. 

<details>
  <summary>**Tips**</summary>
Adapt the code remembering to change the names of the variables and dataset.  
You may have to wrangle your data if a variable is not in the format a function expects it to be in. For example, remove units such as days or change text into categorical data.

</details>

<br>

<br>

[Source](https://github.com/acriach/BLGY5112M)  
[CC Licensed](https://creativecommons.org/)

Adapted from [EnvironmentalComputing](https://github.com/nicercode/EnvironmentalComputing). Authors: Shinichi Nakagawa and Malgorzata (Losia) Lagisz